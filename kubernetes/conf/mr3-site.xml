<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>

<property>
  <name>mr3.runtime</name>
  <value>tez</value>
</property>

<property>
  <name>mr3.cluster.use.hadoop-libs</name>
  <value>false</value>
</property>

<property>
  <name>mr3.lib.uris</name>
  <value></value>
</property>

<property>
  <name>mr3.aux.uris</name>
  <value></value>
</property>

<property>
  <name>mr3.am.launch.cmd-opts</name>
  <value>-server -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -XX:+UseNUMA -XX:+UseG1GC -XX:+ResizeTLAB -Dlog4j.configurationFile=k8s-mr3-container-log4j2.properties -Djavax.security.auth.useSubjectCredsOnly=false -Djava.security.auth.login.config=/opt/mr3-run/conf/jgss.conf -Djava.security.krb5.conf=/opt/mr3-run/conf/krb5.conf -Dsun.security.jgss.debug=true -Djavax.net.ssl.trustStore=/opt/mr3-run/key/hivemr3-ssl-certificate.jks -Djavax.net.ssl.trustStoreType=jks</value>
</property>

<property>
  <name>mr3.am.staging-dir</name>
  <value></value>
</property>

<property>
  <name>mr3.am.generate.dag.graph.viz</name>
  <value>false</value>
</property>

<property>
  <name>mr3.container.kill.policy</name>
  <value>container.kill.wait.workervertex</value>
  <description>
    container.kill.wait.workervertex: wait until WorkerVertexes terminate
    container.kill.nowait: kill without waiting 
  </description>
</property>

<property>
  <name>mr3.am.max.num.concurrent.dags</name>
  <value>32</value>
</property>

<property>
  <name>mr3.dag.priority.scheme</name>
  <value>fifo</value>
</property>
 
<property>
  <name>mr3.am.task.max.failed.attempts</name>
  <value>2</value>
</property>

<property>
  <name>mr3.am.task.retry.on.fatal.error</name>
  <value>true</value>
</property>

<property>
  <name>mr3.am.client.thread-count</name>
  <value>16</value>
</property>

<property>
  <name>mr3.async.logging</name>
  <value>true</value>
</property>

<property>
  <name>mr3.am.permit.custom.user.class</name>
  <value>false</value>
</property>

<!-- resource scheduler -->

<property>
  <name>mr3.am.resourcescheduler.max.requests.per.taskscheduler</name>
  <value>1000</value>
</property>

<!-- container -->

<property>
  <name>mr3.container.launch.cmd-opts</name>
  <value>-XX:+AlwaysPreTouch -Xss512k -XX:+UseG1GC -XX:TLABSize=8m -XX:+ResizeTLAB -XX:+UseNUMA -XX:+AggressiveOpts -XX:InitiatingHeapOccupancyPercent=40 -XX:G1ReservePercent=20 -XX:MaxGCPauseMillis=200 -XX:MetaspaceSize=1024m -server -Djava.net.preferIPv4Stack=true -XX:NewRatio=8 -Dlog4j.configurationFile=k8s-mr3-container-log4j2.properties</value>
  <!-- -XX:SoftRefLRUPolicyMSPerMB=25 --> 
</property>

<property>
  <name>mr3.container.reuse</name>
  <value>true</value>
</property>

<property>
  <name>mr3.container.stop.cross.dag.reuse</name>
  <value>true</value>
</property>

<property>
  <name>mr3.container.idle.timeout.ms</name>
  <value>3600000</value>
</property>

<property>
  <name>mr3.container.wait.final.message.sent</name>
  <value>false</value>
</property>

<property>
  <name>mr3.heartbeat.task.timeout.ms</name>
  <value>60000</value>
</property>

<property>
  <name>mr3.heartbeat.container.timeout.ms</name>
  <value>300000</value>
</property>

<property>
  <name>mr3.am.node-blacklisting.enabled</name>
  <value>false</value>
</property>

<property>
  <name>mr3.am.maxtaskfailure.percent</name>
  <value>1</value>
</property>

<property>
  <name>mr3.container.termination.checker.timeout.ms</name>
  <value>300000</value>
</property>

<!-- Kubernetes -->

<!-- mr3.master.mode is set in mr3-setup.sh --> 

<property>
  <name>mr3.am.resource.memory.mb</name>
  <value>16384</value>
</property>

<property>
  <name>mr3.am.resource.cpu.cores</name>
  <value>2</value>
</property>

<property>
  <name>mr3.am.local.resourcescheduler.max.memory.mb</name>
  <value>8192</value>
</property>

<property>
  <name>mr3.am.local.resourcescheduler.max.cpu.cores</name>
  <value>8</value>
</property>

<property>
  <name>mr3.am.worker.mode</name>
  <!-- <value>local</value> -->
  <value>kubernetes</value>
</property>

<property>
  <name>mr3.container.resourcescheduler.type</name>
  <!-- <value>local</value> -->
  <value>kubernetes</value>
</property>

<!-- should be revised if mr3.master.mode == MR3_MASTER_MODE_KUBERNETES = "kubernetes" -->
<property>
  <name>mr3.am.launch.env</name>
  <value>LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HADOOP_HOME/lib/native/</value>
</property>

<property>
  <name>mr3.container.launch.env</name>
  <value>LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/mr3-run/hadoop/apache-hadoop/lib/native</value>
</property>

<property>
  <name>mr3.am.delete.local.working-dir</name>
  <value>false</value>
</property>

<property>
  <name>mr3.am.local.working-dir</name>
  <value>/opt/mr3-run/am-local-dir/am-local-working-dir</value>
</property>

<property>
  <name>mr3.am.local.log-dir</name>
  <value>/opt/mr3-run/am-local-dir/am-local-log-dir</value>
</property>

<!-- These variables are all set in mr3/mr3-setup.sh:
  mr3.cluster.additional.classpath 
  mr3.principal
  mr3.keytab
  mr3.token.renewal.hdfs.enabled
  mr3.token.renewal.hive.enabled
 -->

<property>
  <name>mr3.token.renewal.pass.credentials.via.memory</name>
  <value>true</value>
</property>

<property>
  <name>mr3.am.token.renewal.paths</name>
  <value>hdfs://red0:8020/user/hive/</value>
</property>

<!-- These variables are all set in mr3/mr3-setup.sh (for Kubernetes):
  mr3.k8s.namespace
  mr3.k8s.pod.master.serviceaccount
  mr3.k8s.pod.master.image
  mr3.k8s.pod.master.user
  mr3.k8s.master.working.dir
  mr3.k8s.master.persistentvolumeclaim.mounts
  mr3.k8s.pod.worker.image
  mr3.k8s.pod.worker.user
  mr3.k8s.worker.working.dir
  mr3.k8s.java.io.tmpdir
  mr3.k8s.worker.persistentvolumeclaim.mounts
  mr3.k8s.conf.dir.configmap
  mr3.k8s.conf.dir.mount.dir
  mr3.k8s.keytab.secret
  mr3.k8s.keytab.mount.dir
  mr3.k8s.keytab.mount.file
 -->

<property>
  <name>mr3.k8s.master.command</name>
  <value>/opt/mr3-run/hive/run-master.sh</value>
</property>

<property>
  <name>mr3.k8s.worker.command</name>
  <value>/opt/mr3-run/hive/run-worker.sh</value>
</property>

<property>
  <name>mr3.k8s.worker.total.max.memory.gb</name>
  <value>1024000</value>
</property>

<property>
  <name>mr3.k8s.worker.total.max.cpu.cores</name>
  <value>1024000</value>
</property>

<property>
  <name>mr3.k8s.api.server.url</name>
  <value>https://kubernetes.default.svc</value>
</property>

<property>
  <name>mr3.k8s.nodes.polling.interval.ms</name>
  <value>60000</value>
</property>

<property>
  <name>mr3.k8s.pods.polling.interval.ms</name>
  <value>15000</value>
</property>

<property>
  <name>mr3.k8s.pod.creation.timeout.ms</name>
  <value>60000</value>
</property>

<property>
  <name>mr3.k8s.pod.master.node.selector</name>
  <value></value>
</property>

<property>
  <name>mr3.k8s.master.pod.affinity.match.label</name>
  <value>hivemr3_app=hiveserver2</value>
</property>

<property>
  <name>mr3.k8s.pod.worker.node.selector</name>
  <value></value>
</property>

<property>
  <name>mr3.k8s.pod.image.pull.policy</name>
  <value>Always</value>
</property>

<property>
  <name>mr3.k8s.pod.image.pull.secrets</name>
  <value></value>
</property>

<property>
  <name>mr3.k8s.host.aliases</name>
  <value>gold0=10.1.90.9,red0=10.1.91.4,indigo20=10.1.91.41</value>
</property>

<property>
  <name>mr3.k8s.pod.master.emptydirs</name>
  <value>/opt/mr3-run/work-local-dir</value>
</property>

<!--
<property>
  <name>mr3.k8s.pod.master.hostpaths</name>
  <value></value>
</property>
 -->

<!--
<property>
  <name>mr3.k8s.pod.worker.emptydirs</name>
  <value>/opt/mr3-run/work-local-dir</value>
</property>
 -->

<property>
  <name>mr3.k8s.pod.worker.hostpaths</name>
  <value>/data1/k8s,/data2/k8s,/data3/k8s,/data4/k8s,/data5/k8s,/data6/k8s</value>
</property>

<property>
  <name>mr3.k8s.master.local.dir.persistentvolumes</name>
  <value></value>
</property>

<!--
<property>
  <name>mr3.k8s.worker.local.dir.persistentvolumes</name>
  <value>/opt/mr3-run/disk1,/opt/mr3-run/disk2</value>
</property>
 -->

<property>
  <name>mr3.k8s.local.dir.persistentvolume.storageclass</name>
  <value>gp2</value>
</property>

<property>
  <name>mr3.k8s.local.dir.persistentvolume.storage</name>
  <value>2Gi</value>
</property>

<property>
  <name>mr3.k8s.mount.keytab.secret</name>
  <value>false</value>
</property>

<property>
  <name>mr3.app.history.logging.enabled</name>
  <value>false</value>
</property>

<property>
  <name>mr3.dag.history.logging.enabled</name>
  <value>false</value>
</property>

<property>
  <name>mr3.task.history.logging.enabled</name>
  <value>false</value>
</property>

<property>
  <name>mr3.container.termination.checker.timeout.ms</name>
  <value>300000</value>
</property>

<property>
  <name>mr3.container.task.failure.num.sleeps</name>
  <value>2</value>
</property>

<!-- auto-scaling -->

<property>
  <name>mr3.enable.auto.scaling</name>
  <value>false</value>
</property>

<property>
  <name>mr3.memory.usage.check.scheme</name>
  <value>average</value>
</property>

<property>
  <name>mr3.auto.scale.out.threshold.percent</name>
  <value>80</value>
</property>

<property>
  <name>mr3.auto.scale.in.threshold.percent</name>
  <value>50</value>
</property>

<property>
  <name>mr3.memory.usage.check.window.length.secs</name>
  <value>600</value>
</property>

<property>
  <name>mr3.check.memory.usage.event.interval.secs</name>
  <value>10</value>
</property>

<property>
  <name>mr3.auto.scale.out.grace.period.secs</name>
  <value>300</value>
</property>

<property>
  <name>mr3.auto.scale.in.delay.after.scale.out.secs</name>
  <value>60</value>
</property>

<property>
  <name>mr3.auto.scale.in.grace.period.secs</name>
  <value>300</value>
</property>

<property>
  <name>mr3.auto.scale.in.wait.dag.finished</name>
  <value>true</value>
</property>

<property>
  <name>mr3.auto.scale.out.num.increment.containers</name>
  <value>1</value>
</property>

<property>
  <name>mr3.auto.scale.in.num.decrement.hosts</name>
  <value>1</value>
</property>

<property>
  <name>mr3.auto.scale.in.min.hosts</name>
  <value>1</value>
</property>

</configuration>
